<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Human-Level Control Through Deep Reinforcement Learning</title>
    <meta name="description" content="Presentation on Human-Level Control Through Deep Reinforcement Learning">
    <meta name="author" content="Kai Arulkumaran">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/css/reveal.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/css/theme/black.min.css" id="theme">
    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement('link');
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match(/print-pdf/gi) ? 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/css/print/pdf.min.css' : 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/css/print/paper.min.css';
      document.getElementsByTagName('head')[0].appendChild(link);
    </script>
  </head>

  <body>
    <div class="reveal">
      <div class="slides">
        <section data-background-video="seaquest.mp4" data-background-video-loop data-background-color="#000000">
          <div style="background-color: rgba(0, 0, 100, 0.8); padding: 40px;">
            <h2 style="font-size: 1.5em;">Human-Level Control Through Deep Reinforcement Learning</h2>
            <p style="font-size: 0.7em;">V. Mnih, K. Kavukcuoglu,	D. Silver,	A. A. Rusu,	J. Veness, M. G. Bellemare,	A. Graves, M. Riedmiller,	A. K. Fidjeland, G. Ostrovski, S. Petersen,	C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra,	S. Legg &amp; D. Hassabis</p>
            <p style="font-size: 0.8em;">Presented by <a href="http://kaixhin.com">Kai Arulkumaran</a> / <a href="https://twitter.com/KaiLashArul">@KaiLashArul</a></p>
            <p style="font-size: 0.8em;">2016-01-26</p>
          </div>
        </section>

        <section>
          <h3>Summary</h3>
          <p>RL + Q-learning</p>
          <p>Deep Q-networks</p>
          <p>Experience replay memory</p>
          <p>Double Q-learning</p>
          <p>Dueling network architectures</p>
        </section>

        <section>
          <h3>Reinforcement Learning 1/2</h3>
          <p><em>Agent</em> in an <em>environment</em> wants to max its cumulative <em>reward</em></p>
          <p>Learns through trial and error (no labels, just reward)</p>
          <p>Reward is often sparse and delayed</p>
          <p>Formulation: Set of states $S$ and set of actions $A$</p>
          <p>Policy $\pi$ determines action $\mathbf{a}_t$ to perform given the state $\mathbf{s}_t$</p>
          <p>Action $\mathbf{a}_t$ transitions $\mathbf{s}_t$ to $\mathbf{s}_{t+1}$ with scalar reward $r_{t+1}$</p>
        </section>

        <section>
          <h3>Reinforcement Learning 2/2</h3>
          <p>Problem may terminate (<em>episodic</em>), or not ($T = \infty$)</p>
          <p>Cumulative reward is called the return $R = \sum\limits_{t=1}^T \gamma^{t-1}r_t$</p>
          <p>Discount $\gamma$ in $[0, 1)$ makes future events less important</p>
          <p>Goal: Maximise expected return given policy $\mathbb{E}[R|\pi]$</p>
        </section>

        <section>
          <h3>Q-Learning 1/2</h3>
          <p>Goal: Learn action-value function $Q(\mathbf{s}, \mathbf{a}) = \mathbb{E}[R|\mathbf{s}, \mathbf{a}]$</p>
          <p>Can define state-value function $V(\mathbf{s}) = \max\limits_{\mathbf{a}}Q(\mathbf{s}, \mathbf{a})$</p>
          <p>With optimal function $Q^*$, simply pick $\arg\!\max$ action</p>
          <p>Learn from experience (sample) at every step (bootstrap)</p>
          <p>Update $Q$ based on error $\delta$, along with a learning rate $\alpha$</p>
        </section>

        <section>
          <h3>Q-Learning 2/2</h3>
          <p>Update rule: $Q_{t+1}(\mathbf{s}_t, \mathbf{a}_t) = Q_t(\mathbf{s}_t, \mathbf{a}_t) + \alpha \delta$</p>
          <p>Temporal difference error $\delta = Y - Q_t$;<br>difference between a target $Y$ and current value of $Q$</p>
          <p>Target $Y = r_{t+1} + \gamma\max\limits_{\mathbf{a}}Q_t(\mathbf{s}_{t+1}, \mathbf{a})$;<br>reward received plus $\max$ discounted estimated Q-value</p>
          <p>Reminder: $\max$ estimated Q-value is $V(\mathbf{s}_{t+1})$</p>
        </section>

        <section>
          <h3>Function Approximation + NNs</h3>
          <p>Impractical to implement a lookup table for large $S$ or $A$</p>
          <p>TD-Gammon</p>
          <p>Neural fitted Q-iteration</p>
        </section>

        <section>
          <h3>Deep Q-Networks</h3>
          <p>$S$ can be large and continuous, but $A$ must be discrete</p>
          <p>Process screen with standard convolutional layers</p>
          <p>Don't input $a_t$, output $Q(\mathbf{s}, a)\ \forall a\in A$</p>
          <p>Efficient - one forward pass for all Q-values</p>
          <p>Utilises same learnt spatial representations</p>
          <p>Two more additions to boost performance...</p>
        </section>

        <section>
          <h3>Target Network</h3>
          <p>Function approximation in RL is unstable</p>
          <p>Use separate <em>policy</em> and <em>target</em> networks</p>
          <p>Policy networks acts; weights $\theta_{policy}$ change rapidly</p>
          <p>Target network evaluates $Q(\mathbf{s}_{t+1}, a; \theta_{target})$ for $Y$</p>
          <p>Update $\theta_{target} \leftarrow \theta_{policy}$ slowly</p>
        </section>

        <section>
          <h3>Experience Replay</h3>
          <p>"Neuro-inspired" mechanism for replaying memories</p>
          <p>Store $(\mathbf{s}_t, a_t, r_{t+1}, \mathbf{s}_{t+1})$ transitions in a (circular) buffer</p>
          <p>Uniformly sample and perform Q-learning</p>
          <p>Data-efficient and can use minibatch updates</p>
          <p>Breaks strong temporal correlations from sampling online</p>
        </section>

        <section>
          <h3>Double-Q Learning</h3>
          <p>Policy network acts, but target network estimates $V(\mathbf{s}_{t+1})$</p>
          <p>Better estimate if policy network picks $\arg\!\max$ action</p>
          <p>Target network then estimates value with chosen action</p>
          <p>$Y_{DQN} = r_{t+1} + \gamma Q(\mathbf{s}_{t+1}, \arg\!\max\limits_a[Q(\mathbf{s}_{t+1}, a; \theta_{target})]; \theta_{target})$</p>
          <p>$Y_{DDQN} = r_{t+1} + \gamma Q(\mathbf{s}_{t+1}, \arg\!\max\limits_a[Q(\mathbf{s}_{t+1}, a; \theta_{policy})]; \theta_{target})$</p>
        </section>

        <section>
          <h3>Advantage Learning???</h3>
          <p>Advantage function $Adv(\mathbf{s}, \mathbf{a}) = Q(\mathbf{s}, \mathbf{a}) - V(\mathbf{s})$</p>
          <p>$V$ represents importance of being in state $\mathbf{s}$</p>
          <p>$Q$ evaluates importance of different actions chosen from $\mathbf{s}$</p>
          <p>Thus $Adv$ provides a relative measure of actions in $\mathbf{s}$</p>
        </section>

        <section>
          <h3>Dueling Architecture</h3>
          <p>Insight is that sometimes actions don't matter so much</p>
          <p>For bootstrapping in RL, a good estimate of $V$ helps</p>
          <p>Therefore decouple into <em>value</em> and <em>advantage</em> streams</p>
          <img src="duel.png">
        </section>

        <section>
          <h3>Dueling Architecture 2/2</h3>
          <p>Function built into the architecture (e.g. ResNets)</p>
          <p>Can visualise what triggers activations in both streams</p>
          <video src="enduro.mp4" loop data-autoplay></video>
        </section>

        <section>
          <h3>Criticism</h3>
          <p>Shallow reinforcement learning</p>
        </section>

        <section>
          <h2>References</h2>
          <ol>
            <li></li>
          </ol>

          <p>All images/videos Â© Atari, Inc. and Google DeepMind</p>
        </section>
      </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/lib/js/head.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/js/reveal.min.js"></script>
    <script>
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        // reveal.js plugins
        dependencies: [
          {src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/plugin/math/math.min.js', async: true} // MathJax
        ]
      });
    </script>
  </body>
</html>
